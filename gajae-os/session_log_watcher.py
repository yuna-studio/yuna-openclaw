#!/usr/bin/env python3
"""
OpenClaw Session Log Watcher
- 1ë¶„ ì£¼ê¸°ë¡œ ì„¸ì…˜ JSONL íŒŒì¼ í´ë§
- <think> ë¸”ë¡, ì‹œìŠ¤í…œ ë¡œê·¸(session, model_change, thinking_level_change, custom) í•„í„°ë§
- user/assistant ë©”ì‹œì§€ë§Œ Firestoreì— ì—…ë¡œë“œ
"""

import os
import sys
import json
import glob
import time
import re
import hashlib
import atexit
from datetime import datetime

import firebase_admin
from firebase_admin import credentials, firestore

# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
AGENTS_DIR = os.path.expanduser("~/.openclaw/agents")
POLL_INTERVAL = 1  # seconds
FIRESTORE_COLLECTION = "chat_logs"
PID_FILE = os.path.expanduser("~/.openclaw/session_log_watcher.pid")
OFFSET_FILE = os.path.expanduser("~/.openclaw/session_log_watcher.offsets.json")

# í•„í„°ë§í•  ì‹œìŠ¤í…œ íƒ€ì…
SKIP_TYPES = {"session", "model_change", "thinking_level_change", "custom"}

# ê° íŒŒì¼ë³„ ë§ˆì§€ë§‰ìœ¼ë¡œ ì½ì€ ë°”ì´íŠ¸ ìœ„ì¹˜ ì¶”ì 
file_offsets: dict[str, int] = {}


# â”€â”€ Offset Persistence â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_offsets():
    """Load saved file offsets from disk."""
    global file_offsets
    try:
        with open(OFFSET_FILE, "r") as f:
            file_offsets = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        file_offsets = {}


def save_offsets():
    """Save file offsets to disk."""
    with open(OFFSET_FILE, "w") as f:
        json.dump(file_offsets, f)


def init_offsets_to_eof():
    """First run: set all offsets to end-of-file so we only watch new content."""
    pattern = os.path.join(AGENTS_DIR, "*/sessions/*.jsonl")
    for path in glob.glob(pattern):
        if path not in file_offsets:
            file_offsets[path] = os.path.getsize(path)
    save_offsets()


# â”€â”€ PID Lock â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def check_pid_lock():
    """Prevent duplicate execution via PID file."""
    if os.path.exists(PID_FILE):
        try:
            with open(PID_FILE, "r") as f:
                old_pid = int(f.read().strip())
            # Check if process is still alive
            os.kill(old_pid, 0)
            print(f"âŒ Already running (PID {old_pid}). Exiting.")
            sys.exit(1)
        except (ProcessLookupError, ValueError):
            # Process is dead, stale PID file
            pass
        except PermissionError:
            # Process exists but we can't signal it
            print(f"âŒ Already running (PID). Exiting.")
            sys.exit(1)

    # Write our PID
    with open(PID_FILE, "w") as f:
        f.write(str(os.getpid()))

    # Cleanup on exit
    atexit.register(cleanup_pid)


def cleanup_pid():
    """Remove PID file on exit."""
    try:
        os.remove(PID_FILE)
    except FileNotFoundError:
        pass

# â”€â”€ Firebase Init â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SERVICE_ACCOUNT_PATH = os.path.expanduser("~/.openclaw/workspace/.firebase-service-account.json")


def init_firebase():
    """Initialize Firebase Admin SDK."""
    cred_path = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS", SERVICE_ACCOUNT_PATH)
    if os.path.exists(cred_path):
        cred = credentials.Certificate(cred_path)
        firebase_admin.initialize_app(cred)
    else:
        firebase_admin.initialize_app()
    return firestore.client()


# â”€â”€ Filters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def strip_think_blocks(text: str) -> str:
    """Remove <think>...</think> blocks from text.
    Also handles unclosed <think> blocks (no closing tag).
    """
    # First: paired <think>...</think>
    text = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL)
    # Then: unclosed <think> that runs to end of text
    text = re.sub(r"<think>.*$", "", text, flags=re.DOTALL)
    return text.strip()


def strip_final_tags(text: str) -> str:
    """Remove <final>...</final> wrapper, keep inner content."""
    m = re.search(r"<final>(.*?)</final>", text, flags=re.DOTALL)
    if m:
        return m.group(1).strip()
    return text


def strip_inbound_meta(text: str) -> str:
    """Remove OpenClaw inbound metadata blocks from user messages."""
    # Conversation info block
    text = re.sub(
        r"Conversation info \(untrusted metadata\):\s*```json\s*\{[^}]*\}\s*```\s*",
        "", text, flags=re.DOTALL
    ).strip()
    # [Telegram ...] prefix
    text = re.sub(r"^\[Telegram[^\]]*\]\s*", "", text).strip()
    # <file ...>...</file> blocks
    text = re.sub(r"<file[^>]*>.*?</file>", "", text, flags=re.DOTALL).strip()
    # [media attached: ...] lines
    text = re.sub(r"\[media attached:.*?\]\s*", "", text).strip()
    # "To send an image back..." instruction lines
    text = re.sub(r"To send an image back.*?(?:\n|$)", "", text).strip()
    return text


# System-injected user messages to skip
SYSTEM_USER_PATTERNS = [
    r"^Pre-compaction memory flush\.",
    r"^Read HEARTBEAT\.md",
    r"^The conversation history before this point was compacted",
    r"^You are running low on context",
    r"^## Silent Replies",
    r"^## Heartbeats",
    r"^## Runtime",
    r"^## Inbound Context",
    r"^\[system\]",
    r"^<system>",
]
_SYSTEM_USER_RE = re.compile("|".join(SYSTEM_USER_PATTERNS), re.MULTILINE)


def is_system_injected(text: str) -> bool:
    """Detect system-injected messages sent as user role."""
    return bool(_SYSTEM_USER_RE.search(text))


# â”€â”€ Sensitive Data Redaction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# â”€â”€â”€ 1. API í‚¤ / í† í° / ì‹œí¬ë¦¿ íŒ¨í„´ â”€â”€â”€
_API_KEY_PATTERNS = [
    # OpenAI (sk-proj, sk-svcacct ë“± í¬í•¨)
    (r"sk-(?:proj-|svcacct-|ant-)?[A-Za-z0-9_-]{20,}", "[API_KEY]"),
    # Google API / Firebase
    (r"AIza[A-Za-z0-9_-]{30,}", "[GOOGLE_KEY]"),
    # GitHub
    (r"(?:ghp|gho|ghu|ghs|ghr)_[A-Za-z0-9]{30,}", "[GITHUB_TOKEN]"),
    (r"github_pat_[A-Za-z0-9_]{30,}", "[GITHUB_TOKEN]"),
    # Slack
    (r"xox[bpras]-[A-Za-z0-9-]+", "[SLACK_TOKEN]"),
    # AWS
    (r"AKIA[A-Z0-9]{16}", "[AWS_KEY]"),
    (r"(?:aws_secret_access_key|aws_session_token)\s*[=:]\s*\S+", "[AWS_SECRET]"),
    # GitLab
    (r"glpat-[A-Za-z0-9_-]{20,}", "[GITLAB_TOKEN]"),
    # npm
    (r"npm_[A-Za-z0-9]{30,}", "[NPM_TOKEN]"),
    # Telegram bot token (ìˆ«ì:ì˜ë¬¸, suffix 34-43ì)
    (r"(?<![A-Za-z0-9])\d{8,10}:[A-Za-z0-9_-]{34,43}(?![A-Za-z0-9])", "[TELEGRAM_BOT_TOKEN]"),
    # Discord bot token (base64-ish)
    (r"[MN][A-Za-z0-9]{23,}\.[A-Za-z0-9_-]{6}\.[A-Za-z0-9_-]{27,}", "[DISCORD_TOKEN]"),
    # Firebase/GCP service account private key fragment
    (r"-----BEGIN (?:RSA )?PRIVATE KEY-----[\s\S]*?-----END (?:RSA )?PRIVATE KEY-----", "[PRIVATE_KEY]"),
    # JWT (3-part base64 dot-separated, 100+ chars)
    (r"eyJ[A-Za-z0-9_-]{20,}\.eyJ[A-Za-z0-9_-]{20,}\.[A-Za-z0-9_-]{20,}", "[JWT_TOKEN]"),
    # Stripe
    (r"(?:sk|pk|rk)_(?:live|test)_[A-Za-z0-9]{20,}", "[STRIPE_KEY]"),
    # Supabase / generic long hex tokens
    (r"sbp_[A-Za-z0-9]{30,}", "[SUPABASE_TOKEN]"),
    # Vercel
    (r"vercel_[A-Za-z0-9_-]{20,}", "[VERCEL_TOKEN]"),
    # Cloudflare
    (r"cf_[A-Za-z0-9_-]{30,}", "[CF_TOKEN]"),
    # Generic long hex secret (40+ chars, likely a key)
    (r"\b[0-9a-f]{40,}\b", "[HEX_SECRET]"),
]

# â”€â”€â”€ 2. ì´ë©”ì¼ â”€â”€â”€
_EMAIL_RE = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")

# â”€â”€â”€ 3. ì „í™”ë²ˆí˜¸ â”€â”€â”€
_PHONE_PATTERNS = re.compile(
    r"(?:"
    # í•œêµ­ íœ´ëŒ€í°: 010-1234-5678, 010 1234 5678, 01012345678
    r"01[016789][-.\s]?\d{3,4}[-.\s]?\d{4}"
    r"|"
    # í•œêµ­ ìœ ì„ : 02-1234-5678, 031-123-4567 ë“±
    r"0\d{1,2}[-.\s]?\d{3,4}[-.\s]?\d{4}"
    r"|"
    # êµ­ì œ í˜•ì‹: +82-10-1234-5678, +1-234-567-8901
    r"\+\d{1,3}[-.\s]?\d{1,4}[-.\s]?\d{3,4}[-.\s]?\d{3,4}"
    r")"
)

# â”€â”€â”€ 4. IP ì£¼ì†Œ â”€â”€â”€
_IP_RE = re.compile(r"\b(?:\d{1,3}\.){3}\d{1,3}\b")
_SAFE_IPS = {"127.0.0.1", "0.0.0.0", "255.255.255.255"}

# â”€â”€â”€ 5. ì ˆëŒ€ ê²½ë¡œ â”€â”€â”€
_ABS_PATH_RE = re.compile(
    r"(?<!\w)"
    r"((?:/[A-Za-z0-9._~@-]+){2,})"
    r"(?!\w)"
)

# â”€â”€â”€ 6. ë‹¨ë… íŒŒì¼ëª… (ê²½ë¡œ ì—†ì´ í™•ì¥ì ë¶™ì€ íŒŒì¼ëª…) â”€â”€â”€
# ì•: ìŠ¬ë˜ì‹œë‚˜ ASCII ë‹¨ì–´ë¬¸ì ë’¤ê°€ ì•„ë‹Œ (í•œê¸€ ë’¤ëŠ” OK)
# ë’¤: ASCII ë‹¨ì–´ë¬¸ì ì•ì´ ì•„ë‹Œ (í•œê¸€ ì•ì€ OK)
_SENSITIVE_FILE_EXTS = [
    "json", "jsonl", "key", "pem", "p12", "pfx",
    "env", "cfg", "conf", "ini", "yml", "yaml", "toml",
    "sh", "bash", "zsh", "py", "js", "ts", "tsx", "jsx",
    "log", "csv", "db", "sqlite", "sql",
    "cert", "crt", "secret", "bak",
    "tar", "gz", "zip", "rar",
]
_STANDALONE_FILE_RE = re.compile(
    r"(?<![/A-Za-z0-9_])"
    r"(\.?[A-Za-z0-9_.-]+\.(?:" + "|".join(_SENSITIVE_FILE_EXTS) + r"))"
    r"(?![A-Za-z0-9_])"
)

# íŒŒì¼ëª… í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ (ë§ˆìŠ¤í‚¹ ì œì™¸ â€” ì½”ë“œ ëŒ€í™”ì—ì„œ ìì£¼ ì“°ì´ëŠ” ì¼ë°˜ íŒŒì¼ëª…)
_FILE_WHITELIST = {
    "package.json", "tsconfig.json", "setup.py", "requirements.txt",
    "Makefile", "Dockerfile", "docker-compose.yml", "docker-compose.yaml",
    ".gitignore", ".env.example", "README.md", "LICENSE",
    "index.js", "index.ts", "main.py", "app.py", "app.js",
    "SKILL.md", "AGENTS.md", "SOUL.md", "USER.md", "MEMORY.md",
    "HEARTBEAT.md", "TOOLS.md", "IDENTITY.md", "BOOTSTRAP.md",
    "firestore.rules", "firebase.json",
}

# íŒŒì¼ í™•ì¥ì (ê²½ë¡œ ë‚´ í™•ì¥ì ê°ì§€ìš©)
_SENSITIVE_EXTENSIONS = {
    ".json", ".jsonl", ".key", ".pem", ".p12", ".pfx", ".env",
    ".cfg", ".conf", ".ini", ".yml", ".yaml", ".toml",
    ".sh", ".bash", ".zsh", ".py", ".js", ".ts", ".tsx", ".jsx",
    ".log", ".csv", ".db", ".sqlite", ".sql",
    ".cert", ".crt", ".secret", ".bak",
    ".tar", ".gz", ".zip", ".rar",
}

# â”€â”€â”€ 7. key=value ì‹œí¬ë¦¿ â”€â”€â”€
_KEY_VALUE_RE = re.compile(
    r"(?i)(password|passwd|secret|token|api_key|apikey|api[-_]?secret"
    r"|auth|credential|private_key|access_key|client_secret"
    r"|db_password|database_url|redis_url|mongo_uri|connection_string)"
    r"\s*[=:]\s*"
    r"['\"]?([^\s'\"]{4,})['\"]?"
)

# â”€â”€â”€ 8. í”„ë¡œì íŠ¸ë³„ ì»¤ìŠ¤í…€ ì¹˜í™˜ â”€â”€â”€
_PROJECT_REDACTIONS: dict[str, str] = {
    # "my-secret-project-id": "[PROJECT_ID]",
}


def redact_sensitive(text: str) -> str:
    """ë¯¼ê° ì •ë³´ë¥¼ ë§ˆìŠ¤í‚¹í•œë‹¤."""

    # 1) API í‚¤ / í† í° (ê°€ì¥ ë¨¼ì € â€” ë‹¤ë¥¸ íŒ¨í„´ì— ê±¸ë¦¬ê¸° ì „ì—)
    for pattern, replacement in _API_KEY_PATTERNS:
        text = re.sub(pattern, replacement, text)

    # 2) key=value ì‹œí¬ë¦¿
    text = _KEY_VALUE_RE.sub(lambda m: f"{m.group(1)}=[REDACTED]", text)

    # 3) ì´ë©”ì¼
    text = _EMAIL_RE.sub("[EMAIL]", text)

    # 4) ì „í™”ë²ˆí˜¸
    text = _PHONE_PATTERNS.sub("[PHONE]", text)

    # 5) IP ì£¼ì†Œ (ì•ˆì „í•œ ê²ƒ ì œì™¸)
    def _mask_ip(m):
        ip = m.group(0)
        if ip in _SAFE_IPS:
            return ip
        return "[IP_ADDR]"
    text = _IP_RE.sub(_mask_ip, text)

    # 6) ì ˆëŒ€ ê²½ë¡œ â†’ ì¶•ì•½
    def _mask_path(m):
        path = m.group(1)
        parts = path.rsplit("/", 1)
        basename = parts[-1] if len(parts) > 1 else parts[0]
        _, ext = os.path.splitext(basename)

        # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ íŒŒì¼ì€ ìœ ì§€
        if basename in _FILE_WHITELIST:
            return basename

        # í™•ì¥ìê°€ ë¯¼ê° ëª©ë¡ì— ìˆìœ¼ë©´ â†’ â€¦/filename.ext
        if ext.lower() in _SENSITIVE_EXTENSIONS:
            return f"â€¦/{basename}"

        # /Users/xxx ë˜ëŠ” /home/xxx ë¡œ ì‹œì‘í•˜ë©´ â†’ í™ˆ ê²½ë¡œ ì¶•ì•½
        home_match = re.match(r"^/(Users|home)/[^/]+", path)
        if home_match:
            remainder = path[len(home_match.group(0)):]
            segs = [s for s in remainder.split("/") if s]
            if len(segs) > 2:
                return "â€¦/" + "/".join(segs[-2:])
            elif segs:
                return "â€¦/" + "/".join(segs)
            else:
                return "~/â€¦"

        return path

    text = _ABS_PATH_RE.sub(_mask_path, text)

    # 7) ë‹¨ë… íŒŒì¼ëª… (í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì œì™¸)
    def _mask_file(m):
        fname = m.group(1)
        if fname in _FILE_WHITELIST:
            return fname
        return f"[FILE:{os.path.splitext(fname)[1]}]"

    text = _STANDALONE_FILE_RE.sub(_mask_file, text)

    # 7b) dotenv íŒŒì¼ (.env, .env.local, .env.production ë“±)
    text = re.sub(
        r"(?<![/A-Za-z0-9_])(\.env(?:\.[A-Za-z0-9_]+)?)(?![A-Za-z0-9_.])",
        lambda m: m.group(1) if m.group(1) in _FILE_WHITELIST else "[DOTENV]",
        text,
    )

    # 8) í”„ë¡œì íŠ¸ë³„ ì»¤ìŠ¤í…€ ì¹˜í™˜
    for keyword, replacement in _PROJECT_REDACTIONS.items():
        text = text.replace(keyword, replacement)

    return text


def extract_text(content) -> str:
    """Extract plain text from message content (string or list of parts)."""
    if isinstance(content, str):
        return content
    if isinstance(content, list):
        parts = []
        for part in content:
            if isinstance(part, dict) and part.get("type") == "text":
                parts.append(part.get("text", ""))
            elif isinstance(part, str):
                parts.append(part)
        return "\n".join(parts)
    return str(content)


def should_skip(entry: dict) -> bool:
    """Return True if this log entry should be filtered out."""
    entry_type = entry.get("type", "")
    if entry_type in SKIP_TYPES:
        return True
    return False


def clean_message(entry: dict) -> dict | None:
    """Clean a message entry. Returns None if it should be skipped."""
    if should_skip(entry):
        return None

    msg = entry.get("message", {})
    role = msg.get("role", "")
    content = msg.get("content", "")

    if not role or not content:
        return None

    text = extract_text(content)

    # Only user and assistant
    if role not in ("user", "assistant"):
        return None

    # Skip system-injected messages disguised as user role
    if role == "user" and is_system_injected(text):
        return None

    # Clean metadata from user messages
    if role == "user":
        text = strip_inbound_meta(text)

    # Strip <think> and <final> tags from assistant messages
    if role == "assistant":
        text = strip_think_blocks(text)
        text = strip_final_tags(text)

    # Skip empty after cleaning
    if not text.strip():
        return None

    # Skip heartbeat / NO_REPLY
    if text.strip() in ("HEARTBEAT_OK", "NO_REPLY"):
        return None

    # ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹
    text = redact_sensitive(text)

    return {
        "role": role,
        "content": text.strip(),
        "timestamp": entry.get("timestamp"),
        "model": entry.get("message", {}).get("model", None),
    }


# â”€â”€ JSONL Processing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_session_files() -> list[tuple[str, str, str]]:
    """Return list of (agent_id, session_id, file_path) tuples."""
    results = []
    pattern = os.path.join(AGENTS_DIR, "*/sessions/*.jsonl")
    for path in glob.glob(pattern):
        parts = path.split(os.sep)
        # .../agents/<agent_id>/sessions/<session_id>.jsonl
        agent_id = parts[-3]
        session_id = os.path.splitext(parts[-1])[0]
        results.append((agent_id, session_id, path))
    return results


def read_new_lines(filepath: str) -> list[str]:
    """Read only new lines since last poll."""
    offset = file_offsets.get(filepath, 0)
    try:
        size = os.path.getsize(filepath)
        if size < offset:
            # File was truncated/rotated, reset
            offset = 0
        with open(filepath, "r") as f:
            f.seek(offset)
            lines = f.readlines()
            file_offsets[filepath] = f.tell()
            save_offsets()
            return lines
    except FileNotFoundError:
        return []


def make_doc_id(entry: dict) -> str:
    """Create deterministic doc ID to avoid duplicates."""
    raw = f"{entry.get('id', '')}-{entry.get('timestamp', '')}"
    return hashlib.md5(raw.encode()).hexdigest()[:16]


# â”€â”€ Main Loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def poll_once(db):
    """Single poll iteration."""
    session_files = get_session_files()
    total_uploaded = 0

    for agent_id, session_id, filepath in session_files:
        new_lines = read_new_lines(filepath)
        if not new_lines:
            continue

        batch = db.batch()
        batch_count = 0

        for line in new_lines:
            line = line.strip()
            if not line:
                continue
            try:
                entry = json.loads(line)
            except json.JSONDecodeError:
                continue

            cleaned = clean_message(entry)
            if cleaned is None:
                continue

            doc_id = make_doc_id(cleaned)
            doc_ref = db.collection(FIRESTORE_COLLECTION).document(doc_id)

            cleaned["agent"] = agent_id
            cleaned["sessionId"] = session_id

            batch.set(doc_ref, cleaned)
            batch_count += 1

        if batch_count > 0:
            batch.commit()
            total_uploaded += batch_count

    if total_uploaded > 0:
        now = datetime.now().strftime("%H:%M:%S")
        print(f"[{now}] Uploaded {total_uploaded} messages")


def main():
    check_pid_lock()
    print(f"ğŸ¦ Session Log Watcher starting... (PID {os.getpid()})")
    print(f"   Agents dir: {AGENTS_DIR}")
    print(f"   Poll interval: {POLL_INTERVAL}s")
    print(f"   Firestore collection: {FIRESTORE_COLLECTION}")
    print()

    db = init_firebase()
    print("âœ… Firebase connected\n")

    # Load saved offsets or init to EOF on first run
    load_offsets()
    if not file_offsets:
        print("ğŸ“Œ First run: skipping existing logs, watching new only")
        init_offsets_to_eof()

    while True:
        try:
            poll_once(db)
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Watcher stopped.")
            sys.exit(0)
        except Exception as e:
            print(f"âŒ Error: {e}", file=sys.stderr)

        time.sleep(POLL_INTERVAL)


if __name__ == "__main__":
    main()
